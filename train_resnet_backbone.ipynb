{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the multimodal neural network with a resnet backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import module for multimodal model for air pollution\n",
    "from models.multimodal_model_resnet import PMModel\n",
    "# import the module to create the multiModal dataset\n",
    "from models.data_loader import AirPollutionDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "# ramdomsplit is used to divide the dataset as train test split\n",
    "from torch.utils.data import random_split\n",
    "from torch import nn\n",
    "from torch import optim \n",
    "from torchvision import  transforms\n",
    "from torch import random\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available(): \n",
    " device = \"cuda\" \n",
    "else: \n",
    " device = \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd9e19f5210>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_seed = 0\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforms\n",
    "transform =transforms.Compose([\n",
    "        #transforms.CenterCrop(10),\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.PILToTensor(),\n",
    "        transforms.RandomVerticalFlip(p=1)\n",
    "        #transforms.Normalize((0.485, 0.456, 0.406,0.406), (0.229, 0.224, 0.225,0.225))\n",
    "\n",
    "        #transforms.ConvertImageDtype(torch.float)\n",
    "        ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the mulyimodal dataset\n",
    "images_dir='data_dir'\n",
    "csv_file='multimodal_dataset.csv'\n",
    "\n",
    "\n",
    "#transform = transforms.Resize(output_size)\n",
    "dataset = AirPollutionDataset(images_dir,csv_file,transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n",
      "7\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# creation of the dataloader\n",
    "# Define the proportions for train, validation, and test sets\n",
    "train_ratio = 0.8# 80% for training\n",
    "val_ratio = 0.1# 10% for validation\n",
    "test_ratio = 0.1 # 10% for testing\n",
    "\n",
    "# Calculate the lengths of each split\n",
    "total_samples = len(dataset)\n",
    "train_size = int(train_ratio * total_samples)\n",
    "val_size = int(val_ratio * total_samples)\n",
    "test_size = total_samples - train_size - val_size\n",
    "\n",
    "# Use random_split to create the splits\n",
    "train_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create DataLoader instances for each split\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "print(len(train_loader))\n",
    "print(len(val_loader))\n",
    "print(len(test_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/armandine/anaconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n",
      "/Users/armandine/anaconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate your model\n",
    "model1 = PMModel(S5no2_num_features=10, S5so2_num_features=10,tabular_input_count=5,tabular_features=1,out=1)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "l2_lambda = 0.1\n",
    "criterion = nn.MSELoss()  # For example, you can change this based on your task\n",
    "optimizer = optim.Adam(model1.parameters(), lr=0.00001)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30] - Train loss : 623.2091674804688  - Val Loss: 773.0000610351562  - Rsquare:-85393.17730617935\n",
      "Epoch [2/30] - Train loss : 618.5892944335938  - Val Loss: 766.1903076171875  - Rsquare:-55690.89927977892\n",
      "Epoch [3/30] - Train loss : 609.8261108398438  - Val Loss: 757.8883056640625  - Rsquare:-39146.15029552828\n",
      "Epoch [4/30] - Train loss : 601.0767211914062  - Val Loss: 748.7943725585938  - Rsquare:-20663.504129380985\n",
      "Epoch [5/30] - Train loss : 610.30908203125  - Val Loss: 742.138671875  - Rsquare:-17712.408955674102\n",
      "Epoch [6/30] - Train loss : 879.0184326171875  - Val Loss: 734.2096557617188  - Rsquare:-12378.682840118025\n",
      "Epoch [7/30] - Train loss : 580.7213134765625  - Val Loss: 727.0372924804688  - Rsquare:-8516.67271004105\n",
      "Epoch [8/30] - Train loss : 567.3833618164062  - Val Loss: 711.0931396484375  - Rsquare:-4779.105089934389\n",
      "Epoch [9/30] - Train loss : 566.7987060546875  - Val Loss: 704.3303833007812  - Rsquare:-3628.7008703512975\n",
      "Epoch [10/30] - Train loss : 550.5092163085938  - Val Loss: 690.7999877929688  - Rsquare:-2030.545113129642\n",
      "Epoch [11/30] - Train loss : 547.4325561523438  - Val Loss: 681.4887084960938  - Rsquare:-1011.2123489089329\n",
      "Epoch [12/30] - Train loss : 814.640625  - Val Loss: 682.6278076171875  - Rsquare:-918.7121382467868\n",
      "Epoch [13/30] - Train loss : 530.306640625  - Val Loss: 671.5228881835938  - Rsquare:-854.9459208750714\n",
      "Epoch [14/30] - Train loss : 508.5012512207031  - Val Loss: 654.0791015625  - Rsquare:-376.4530671523761\n",
      "Epoch [15/30] - Train loss : 499.088623046875  - Val Loss: 635.86328125  - Rsquare:-285.10072011182865\n",
      "Epoch [16/30] - Train loss : 525.2149047851562  - Val Loss: 643.5076904296875  - Rsquare:-585.2590061880355\n",
      "Epoch [17/30] - Train loss : 493.0860290527344  - Val Loss: 625.1409912109375  - Rsquare:-174.52271633939804\n",
      "Epoch [18/30] - Train loss : 486.0258483886719  - Val Loss: 554.1107788085938  - Rsquare:-52.68545327668994\n",
      "Epoch [19/30] - Train loss : 461.0140686035156  - Val Loss: 591.5026245117188  - Rsquare:-84.365764486818\n",
      "Epoch [20/30] - Train loss : 457.3667297363281  - Val Loss: 590.0897827148438  - Rsquare:-96.80661096936767\n",
      "Epoch [21/30] - Train loss : 440.05181884765625  - Val Loss: 538.2610473632812  - Rsquare:-110.74788060920312\n",
      "Epoch [22/30] - Train loss : 434.15093994140625  - Val Loss: 507.890625  - Rsquare:-55.54682285444619\n",
      "Epoch [23/30] - Train loss : 430.626708984375  - Val Loss: 541.9879760742188  - Rsquare:-42.76585898764449\n",
      "Epoch [24/30] - Train loss : 413.1103210449219  - Val Loss: 502.29779052734375  - Rsquare:-10.536096124531467\n",
      "Epoch [25/30] - Train loss : 404.5080261230469  - Val Loss: 479.48724365234375  - Rsquare:-23.20947215476533\n",
      "Epoch [26/30] - Train loss : 389.281005859375  - Val Loss: 466.0909118652344  - Rsquare:-16.213607346709516\n",
      "Epoch [27/30] - Train loss : 381.5542907714844  - Val Loss: 488.48248291015625  - Rsquare:-9.485548080057729\n",
      "Epoch [28/30] - Train loss : 428.16375732421875  - Val Loss: 569.1983032226562  - Rsquare:-3.8507913200189003\n",
      "Epoch [29/30] - Train loss : 358.23602294921875  - Val Loss: 520.387939453125  - Rsquare:-5.996534102941434\n",
      "Epoch [30/30] - Train loss : 378.8457336425781  - Val Loss: 448.56341552734375  - Rsquare:-6.003514525459936\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs =30\n",
    "val_losses=[]\n",
    "train_losses=[]\n",
    "for epoch in range(num_epochs):\n",
    "    model1.train()\n",
    "    avg_trainloss=0.0\n",
    "    total_train_loss=0.0\n",
    "    for batch_data in train_loader:\n",
    "       # print(len(batch_data))\n",
    "        S5no2,S5so2,tabular,labels=batch_data\n",
    "        S5no2=S5no2.float().to(device)\n",
    "        S5so2=S5so2.float().to(device)\n",
    "        tabular=tabular.float().to(device)\n",
    "        labels=labels.float().reshape(-1,1).to(device)\n",
    "        #print(S5no2.shape)\n",
    "        #print(tabular.shape)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "      \n",
    "        outputs = model1(S5no2,S5so2,tabular).float()\n",
    "        #l2_loss = sum(p.norm(2) for p in model1.parameters())\n",
    "        loss = criterion(outputs, labels)\n",
    "        #loss = loss + l2_lambda * l2_loss\n",
    "        total_train_loss+=loss\n",
    "        \n",
    "        #loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_trainloss=total_train_loss/len(train_loader)\n",
    "       \n",
    "    \n",
    "    # Validation\n",
    "    model1.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        val_r=0.0\n",
    "        for batch_data in val_loader:\n",
    "            #print(len(batch_data))\n",
    "            S5no2,S5so2,tabular,labels=batch_data \n",
    "            S5no2=S5no2.float().to(device)\n",
    "            S5so2=S5so2.float().to(device)\n",
    "            tabular=tabular.float().to(device)\n",
    "            labels=labels.float().reshape(-1,1).to(device)\n",
    "            outputs = model1(S5no2,S5so2,tabular)\n",
    "            val_loss += criterion(outputs, labels)\n",
    "            val_r+= r2_score(outputs.cpu(),labels.cpu())\n",
    "            \n",
    "        average_val_loss = val_loss / len(val_loader)\n",
    "        average_val_r=val_r/len(val_loader)\n",
    "        \n",
    "        val_losses.append(average_val_loss.item())\n",
    "        train_losses.append(avg_trainloss.item())\n",
    "        torch.save(model1.state_dict(), f'models/checkpoints/cnn_backbone/model_epoch_{epoch+1}.pth')\n",
    "        \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Train loss : {avg_trainloss}  - Val Loss: {average_val_loss.item()}  - Rsquare:{average_val_r}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.load_state_dict(torch.load('models/checkpoints/resnet_backbone/model_epoch_25.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared score: 0.2183\n",
      "MSE: 369.5028\n",
      "RMSE: 19.2225\n",
      "MAE: 9.1935\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error,mean_squared_error,median_absolute_error\n",
    "model1.eval()\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_data in test_loader:\n",
    "        S5no2,S5so2,tabular,labels=batch_data \n",
    "        S5no2=S5no2.float().to(device)\n",
    "        S5so2=S5so2.float().to(device)\n",
    "        tabular=tabular.float().to(device)\n",
    "        labels=labels.float().reshape(-1,1).to(device)\n",
    "        outputs = model1(S5no2,S5so2,tabular)\n",
    "        \n",
    "        y_true.extend(labels.numpy())\n",
    "        y_pred.extend(outputs.numpy())\n",
    "\n",
    "# Calculate R-squared score\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "mse=mean_squared_error(y_true,y_pred)\n",
    "mae=mean_absolute_error(y_true,y_pred)\n",
    "rmse=mean_squared_error(y_true,y_pred)**0.5\n",
    "\n",
    "print(f\"R-squared score: {r2:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a2fac7082c9fd197cae6c34e59831c2d1cabe6936d70c69517db7d424f3e7ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
